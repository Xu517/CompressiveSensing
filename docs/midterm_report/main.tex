\documentclass[11pt,twocolumn]{article}\usepackage[top=2.2cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}\usepackage{color}\usepackage{url} \title{Midterm Report: Implementing Compressive Sampling Algorithms Across The Cloud}\author{Jason Halpern}\date{October 29, 2012}					\begin{document}\maketitle\section{Motivation}The purpose of this project is to explore various implementations of compressivesampling algorithms and to find ways of parallelizing these algorithms across severalmachines. If we can efficiently parallelize compressive sampling recovery algorithms,then we can handle the decompression of these signals across the cloud, which will leadto a large reduction in bandwidth.  These new approaches will represent a significantadvancement over the way that current wireless sensor networks handle the compressionand decompression of signals.\section{Project Objective}Compressive Sampling is about acquiring and recovering a sparse signal in an efficientmanner. It involves finding the number of measurements that are necessary to reconstructthe signal. The goal of the project is to research and explore several compressivesampling algorithms and to determine which algorithm is the best choice to implement ina distributed manner. 				\section{Choosing a Compressive Sensing Algorithm}When we first started this project, we decided that we were going to choose one specificcompressive sensing recovery algorithm and to focus on implementing that algorithm inJava. After implementing the code in Java, we would later find ways to distribute thatalgorithm across a Hadoop cluster. The first thing I did for the project was to read thepaper, {\em An Introduction to Compressive Sampling} by Candes and Wakin\cite{candes:2008}to get a better grasp on the significance of compressive sampling. I also read other short reports and presentations about compressive sensing to give myself a better background on the topic. The next step was to find a specific compressive sensing algorithm to implement. We mainly evaluated algorithms that we found on the Rice Computer Science website \cite{ricecs} in their model based compressive sensing toolbox. We evaluated the following algorithms in the toolbox: 2D trees,  cosamp, block sparsityand clustered sparsity. I first randomly picked the block sparsity algorithm and closelywalked through the Matlab code line by line to make sure the mathematical complexity ofthe algorithm would not be too difficult for us to understand. I then took a quick lookat the other algorithms to quickly gauge their complexity as well. I also looked at several other factors when evaluating the algorithms. Two of the mainfactors were the amount of documentation in the code and the inclusion of a researchpaper that we could read in conjunction with the code. We chose the cosamp algorithmbecause it was well documented, complete and included the following paper, {\em CoSaMP:iterative signal recovery from complete and inaccurate measurements}, written by Needelland Tropp at the California Institute of Technology \cite{needell:2010}. In addition,from reading the paper we learned that this algorithm is a greedy pursuit and is a goodbalance in terms of its running time and sampling efficiency. Most algorithms tend to beextreme in one of those categories. The CoSaMP algorithm is also the only signalreconstruction algorithm that accomplishes all of the following: 1) It should acceptsamples from a variety of sampling schemes; 2) It should succeed using a minimal numberof samples; 3) It should be robust when samples are contaminated with noise; 4) Itshould provide optimal error guarantees for every target signal; 5) It should offerprovably efficient resource usage. For all of the reasons mentioned above, we chose toimplement the CoSaMP algorithm.\section{Choosing a Java Library}The next step in the process was to pick a Java library that would allow us to carry outthe necessary matrix operations, including matrix multiplication and transpose. Thedifferent libraries that I evaluated were Berkeley’s Colt and Parallel Colt, ojAlgo,Mahout, Matlab Builder JA, Apache Commons Math, JBlas and EJML. The first library Ilooked at was Matlab Builder JA, which transforms Matlab code directly into Java code.Unfortunately, since Columbia does not have a license for this software we were unableto use it. I also decided against using ojAlgo and EJML because there is not a strongcommunity of users around these libraries and they do not seem to be well supportedanymore. I also eliminated JBlas even though it has done very well in benchmarkperformance tests against the other libraries, because it also did not seem to havestrong support. It also was not clear from the documentation if all the operations thatwe needed were included in this library. In the end, I was deciding between Mahout\cite{Mahout} and Apache Commons Math. Both libraries seemed to have all the operationswe needed and they also both have strong developer and user communities. The reason Ichose Mahout is that it is mainly used to carry out distributed computations on top ofHadoop \cite{Hadoop} and since our goal for this project is to distribute thecompressive sensing recovery algorithm, it made sense to use a library that is oftenused for distributed computation. \section{Using Mahout}Mahout \cite{Mahout} is Apache’s open source machine learning project. An interesting aspect of Mahout is that it makes it simple to distribute algorithms such as classification and clustering. In the second phase of the project, we will look into using more of the machine learning features of Mahout \cite{bookmahout} with the Hadoop \cite{Hadoop} framework. Up until this point, we have used the matrix capabilities in Mahout’s Math library in order to migrate the code from Matlab to Java. Mahout has a built-in matrix object that allows us to do operations such as matrix multiplication, addition, subtraction, as well as transpose and other matrix manipulation functions. \section{Migrating Code From Matlab to Java}The major effort in the first phase of the project has been migrating the code we have in Matlab for the cosamp algorithm into Java. This phase of the project required evaluating each line of code in Matlab, determining what operations were being done and then writing the Java code to carry out these functions. Sometimes this simply involved using built-in Mahout functions, but it often required writing functions in Java to carry out equivalent operations to what was being done in Matlab. For example, for the Matlab functions abs(), sort(), zeros(), randperm() and randn(), we had to write and then test functions in Java that did the same thing as these functions in Matlab. There were also many other matrix manipulations functions that involved slicing and rearranging matrices that we had to write in Java. For example, for the following line of Matlab code:	x(v(1:K)) = randn(K,1);I wrote the function in Java called setCellValues() that assigns the values in the randn() matrix to the cells of the matrix x that are indicated by the rows in matrix v.\section{Test Cases}I have used JUnit to write test cases for all the code written so far. As I came across specific functions in Matlab that are not yet implemented in Mahout, I had to first determine what this function did and then I had to write the necessary code in Java to match the function in Matlab. Before writing the function, I would write a test case using JUnit that I could run after writing the function to make sure it was working properly. At the end of this report is a table that includes a Matlab function from the algorithm, a corresponding Java function I have written (in MatrixHelper.java) that is equivalent to the Matlab function and the test case I have written (in SignalTesting.java) for my code. NOTE: Some parameters have been removed to make the table more readable.\section{Timeline}September 27th – Finished analyzing the different compressive sensing recovery algorithms that we had in Matlab. Read the research papers associated with the various compressive sensing algorithms. Decided to implement the CoSamp algorithm.October 4th – Evaluated many different Java libraries for doing matrix operations and decided to use Mahout. Setup Mahout and ran some tests using Mahout’s matrix functionality to become more familiar with the library.October 11th – Started to migrate the code from Matlab to Java and started to write test cases for the new functions I wrote.October 27th – Finished migrating all of the code from Matlab to Java and wrote test cases for all the new functions I have written. I am currently in the process of testing the algorithm from beginning to end to make sure that all of our steps match the corresponding steps in Matlab. I am evaluating the matrices at different points in the algorithm to make sure this is working properly. \section{Next Steps}After we have determined that the algorithm has been fully implemented, we will experiment with ways to distribute the algorithm. First, we will try to do this on a standalone Hadoop machine and then we will try to do this across a Hadoop cluster. Lastly, if we have time we will attempt to use either the Android simulator or sensor networks and collect signals from these devices and then use our distributed algorithm to recover these signals.\bibliographystyle{abbrv}\bibliography{main}\onecolumn\begin{table}[t]\centering\begin{tabular}{|l|l|l|}\hline\textbf{Matlab} & \textbf{Java} & \textbf{JUnit}\\\hlinesort( ,’Descending’) & sortDescending() & sortDescTest()\\\hlineabs() & getAbsMatrix() & absValueTest()\\\hlinene(scosamp,0) & notEqual() & notEqualTest() \\\hlinefind( , 0) & findNonzero() & findNonzerosTest() \\\hlineunion() & union() & unionTest() \\\hline[M,N] = size(Phi) & rowSize()/columnSize() & n/a\\\hlineyy(:) & toSingleColumn() & singleColumnTest() \\\hlineww(1:K) & getIndices() & indexTest() \\\hlinebb2(ttcosamp) = wcosamp & setCellValues() & n/a\\\hlinexcosamp(:,kk) & getColumn() & getColumnTest() \\\hlinePhi(:,ttcosamp) & getColumns() & getColumnsTest() \\\hlinezeros() & fillWithZeros() & n/a\\\hlinexcosamp(:,kk) = scosamp & modifyColumn() & modifyColumnTest() \\\hlinenorm() & norm() & normTest()\\\hlinerandn() & randN() & n/a \\\hlinerandperm() & randomPermutation() & n/a\\\hlinelength() & length() & lengthTest() \\\hlinexcosamp(:,kk+1:end)=[] & removeColumns() & removeColumnsTest() \\\hlinexhat = xcosamp(:,end) & getLastColumn() & getLastColumnTest() \\\hline\end{tabular}\end{table}\begin{table}[t]\centering\begin{tabular}{|l|p{10cm}|}\hline\textbf{Matlab} & \textbf{Description}\\\hlinesort( ,’Descending’) & Sort each column in the matrix so the values are in 						descending order\\\hlineabs() & Converts each element in the matrix to its absolute value\\\hlinene(scosamp,0) & If a cell in a matrix does not equal zero then set its value 				to 1, otherwise zero\\\hlinefind( , 0) & Return a matrix with the row and column location of all nonzero 			 values\\\hlineunion() & Form a vector that is the union of two other vectors\\\hline[M,N] = size(Phi) & M is the number of rows and N is the number of columns\\\hlineyy(:) & Transform an entire matrix into a single column\\\hlineww(1:K) & Return a matrix with columns 1 through K\\\hlinebb2(ttcosamp) = wcosamp & Set the columns indicated in ttcosamp of the bb2 						  matrix to the values in wcosamp\\\hlinexcosamp(:,kk) & Return a column vector of the kk column\\\hlinePhi(:,ttcosamp) & Return a matrix of columns from the Phi matrix as 				  indicated in ttcosamp\\\hlinezeros() & Fill the matrix with all zeros\\\hlinexcosamp(:,kk) = scosamp & Modifies the kk column to the values indicated 						  in scosamp\\\hlinenorm() & The norm function calculates several different types of matrix and 		 vector norms.\\\hlinerandn() & Returns a matrix of pseudorandom values from the standard 		  normal distribution.\\\hlinerandperm() & Returns a row vector of a permutation of integers from 1 to the 			 signal length.\\\hlinelength() & Returns either the number of rows or the number of columns, 		   whichever is higher\\\hlinexcosamp(:,kk+1:end)=[] & Remove all columns between (kk+1) and end\\\hlinexhat = xcosamp(:,end) & Return the last column of a matrix\\\hline\end{tabular}\end{table}\end{document}             % End of document.