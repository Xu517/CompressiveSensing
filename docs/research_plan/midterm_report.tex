\documentclass[11pt,twocolumn]{article}
\usepackage[top=2.2cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}
\usepackage{color}

\title{Midterm Report: Implementing Compressive Sampling Algorithms Across The Cloud}
\author{Jason Halpern}
\date{October 29, 2012}					

\begin{document}
\maketitle

\section{Motivation}

The purpose of this project is to explore various implementations of compressive sampling algorithms and to find ways of parallelizing these algorithms across several machines. If we can efficiently parallelize compressive sampling recovery algorithms, then we can handle the decompression of these signals across the cloud, which will lead to a large reduction in bandwidth.  These new approaches will represent a significant advancement over the way that current wireless sensor networks handle the compression and decompression of signals.

\section{Project Objective}

Compressive Sampling is about acquiring and recovering a sparse signal in an efficient manner. It involves finding the number of measurements that are necessary to reconstruct the signal. The goal of the project is to research and explore several compressive sampling algorithms and to determine which algorithm is the best choice to implement in a distributed manner. 				

\section{Choosing a Compressive Sensing Algorithm}

When we first started this project, we decided that we were going to choose one specific compressive sensing recovery algorithm and to focus on implementing that algorithm in Java. After implementing the code in Java, we would later find ways to distribute that algorithm across a Hadoop cluster. The first thing I did for the project was to read the paper, “An Introduction to Compressive Sampling” by Candes and Wakin to get a better grasp on the significance of compressive sampling. I also read other short reports and presentations about compressive sensing to give myself a better background on the topic.The next step was to find a specific compressive sensing algorithm to implement. We mainly evaluated algorithms that we found on the Rice Computer Science website in their model based compressive sensing toolbox.We evaluated the following algorithms in the toolbox: 2D trees,  cosamp, block sparsity and clustered sparsity. I first randomly picked the block sparsity algorithm and closely walked through the Matlab code line by line to make sure the mathematical complexity of the algorithm would not be too difficult for us to understand. I then took a quick look at the other algorithms to quickly gauge their complexity as well. I also looked at several other factors when evaluating the algorithms. Two of the main factors were the amount of documentation in the code and the inclusion of a research paper that we could read in conjunction with the code. We chose the cosamp algorithm because it was well documented, complete and included the following paper, “ CoSaMP: iterative signal recovery from complete and inaccurate measurements,” written by Needell and Tropp at the California Institute of Technology. In addition, from reading the paper we learned that this algorithm is a greedy pursuit and is a good balance in terms of its running time and sampling efficiency. Most algorithms tend to be extreme in one of those categories. The CoSaMP algorithm is also the only signal reconstruction algorithm that accomplishes all of the following: 1) It should accept samples from a variety of sampling schemes; 2) It should succeed using a minimal number of samples; 3) It should be robust when samples are contaminated with noise; 4) It should provide optimal error guarantees for every target signal; 5) It should offer provably efficient resource usage. For all of the reasons mentioned above, we chose to implement the CoSaMP algorithm.

\section{Choosing a Java Library}

The next step in the process was to pick a Java library that would allow us to carry out the necessary matrix operations, including matrix multiplication and transpose. The different libraries that I evaluated were Berkeley’s Colt and Parallel Colt, ojAlgo, Mahout, Matlab Builder JA, Apache Commons Math, JBlas and EJML. The first library I looked at was Matlab Builder JA, which transforms Matlab code directly into Java code. Unfortunately, since Columbia does not have a license for this software we were unable to use it. I also decided against using ojAlgo and EJML because there is not a strong community of users around these libraries and they do not seem to be well supported anymore. I also eliminated JBlas even though it has done very well in benchmark performance tests against the other libraries, because it also did not seem to have strong support. It also was not clear from the documentation if all the operations that we needed were included in this library. In the end, I was deciding between Mahout and Apache Commons Math. Both libraries seemed to have all the operations we needed and they also both have strong developer and user communities. The reason I chose Mahout is that it is mainly used to carry out distributed computations on top of Hadoop and since our goal for this project is to distribute the compressive sensing recovery algorithm, it made sense to use a library that is often used for distributed computation. 

\section{Using Mahout}

Mahout is Apache’s open source machine learning project. An interesting aspect of Mahout is that it makes it simple to distribute algorithms such as classification and clustering. In the second phase of the project, we will look into using more of the machine learning features of Mahout with the Hadoop framework. Up until this point, we have used the matrix capabilities in Mahout’s Math library in order to migrate the code from Matlab to Java. Mahout has a built-in matrix object that allows us to do operations such as matrix multiplication, addition, subtraction, as well as transpose and other matrix manipulation functions. 

\section{Migrating Code From Matlab to Java}

The major effort in the first phase of the project has been migrating the code we have in Matlab for the cosamp algorithm into Java. This phase of the project required evaluating each line of code in Matlab, determining what operations were being done and then writing the Java code to carry out these functions. Sometimes this simply involved using built-in Mahout functions, but it often required writing functions in Java to carry out equivalent operations to what was being done in Matlab. For example, for the Matlab functions abs(), sort(), zeros(), randperm() and randn(), we had to write and then test functions in Java that did the same thing as these functions in Matlab. There were also many other matrix manipulations functions that involved slicing and rearranging matrices that we had to write in Java. For example, for the following line of Matlab code:	x(v(1:K)) = randn(K,1);I wrote the function in Java called setCellValues() that assigns the values in the randn() matrix to the cells of the matrix x that are indicated by the rows in matrix v.

\section{Test Cases}

I have used JUnit to write test cases for all the code written so far. As I came across specific functions in Matlab that are not yet implemented in Mahout, I had to first determine what this function did and then I had to write the necessary code in Java to match the function in Matlab. Before writing the function, I would write a test case using JUnit that I could run after writing the function to make sure it was working properly. 

\section{Timeline}

September 27th – Finished analyzing the different compressive sensing recovery algorithms that we had in Matlab. Read the research papers associated with the various compressive sensing various algorithms. Decided to implement the CoSamp algorithm.
October 4th – Evaluated many different Java libraries for doing matrix operations and decided to use Mahout. Setup Mahout and ran some tests using Mahout’s matrix functionality to become more familiar with the library.

October 11th – Started to migrate the code from Matlab to Java and started to write test cases for the new functions I wrote.

October 27th – Finished migrating all of the code from Matlab to Java and wrote test cases for all the new functions I have written. I am currently in the process of testing the algorithm from beginning to end to make sure that all of our steps match the corresponding steps in Matlab. I am evaluating the matrices at different points in the algorithm to make sure this is working properly. 

\section{Next Steps}
After we have determined that the algorithm has been fully implemented, we will experiment with ways to distribute the algorithm. First, we will try to do this on a standalone Hadoop machine and then we will try to do this across a Hadoop cluster. Lastly, if we have time we will attempt to use either the Android simulator or sensor networks and collect signals from these devices and then use our distributed algorithm to recover these signals.



\end{document}             % End of document.
